import os
from pathlib import Path

import lightning.pytorch as pl
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchmetrics
import trimesh
from torch.autograd import Variable

from utils.teeth_numbering import label_to_colors


def Generalized_Dice_Loss(y_pred, y_true, smooth=1.0):
    '''
    inputs:
        y_pred [n_classes, x, y, z] probability
        y_true [n_classes, x, y, z] one-hot code
        class_weights
        smooth = 1.0
    '''
    smooth = 1.
    loss = 0.
    y_pred = y_pred.transpose(2, 1)
    n_classes = y_pred.shape[-1]
    y_true = nn.functional.one_hot(y_true, num_classes=n_classes)
    class_weights = torch.ones(n_classes).to(y_pred.device, dtype=torch.float)

    for c in range(0, n_classes):
        pred_flat = y_pred[:, :, c].reshape(-1)
        true_flat = y_true[:, :, c].reshape(-1)
        intersection = (pred_flat * true_flat).sum()

        # with weight
        w = class_weights[c] / class_weights.sum()
        loss += w * (1 - ((2. * intersection + smooth) /
                          (pred_flat.sum() + true_flat.sum() + smooth)))

    return loss


class STN3d(nn.Module):
    def __init__(self, channel):
        super(STN3d, self).__init__()
        self.conv1 = torch.nn.Conv1d(channel, 64, 1)
        self.conv2 = torch.nn.Conv1d(64, 128, 1)
        self.conv3 = torch.nn.Conv1d(128, 1024, 1)
        self.fc1 = nn.Linear(1024, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 9)
        self.relu = nn.ReLU()

        self.bn1 = nn.BatchNorm1d(64)
        self.bn2 = nn.BatchNorm1d(128)
        self.bn3 = nn.BatchNorm1d(1024)
        self.bn4 = nn.BatchNorm1d(512)
        self.bn5 = nn.BatchNorm1d(256)

    def forward(self, x):
        batchsize = x.size()[0]
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.relu(self.bn3(self.conv3(x)))
        x = torch.max(x, 2, keepdim=True)[0]
        x = x.view(-1, 1024)

        x = F.relu(self.bn4(self.fc1(x)))
        x = F.relu(self.bn5(self.fc2(x)))
        x = self.fc3(x)

        iden = Variable(torch.from_numpy(np.array([1, 0, 0, 0, 1, 0, 0, 0, 1]).astype(np.float32))).view(1, 9).repeat(
            batchsize, 1)
        if x.is_cuda:
            iden = iden.to(x.get_device())
        x = x + iden
        x = x.view(-1, 3, 3)
        return x


class STNkd(nn.Module):
    def __init__(self, k=64):
        super(STNkd, self).__init__()
        self.conv1 = torch.nn.Conv1d(k, 64, 1)
        self.conv2 = torch.nn.Conv1d(64, 128, 1)
        self.conv3 = torch.nn.Conv1d(128, 512, 1)
        self.fc1 = nn.Linear(512, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, k * k)
        self.relu = nn.ReLU()

        self.bn1 = nn.BatchNorm1d(64)
        self.bn2 = nn.BatchNorm1d(128)
        self.bn3 = nn.BatchNorm1d(512)
        self.bn4 = nn.BatchNorm1d(256)
        self.bn5 = nn.BatchNorm1d(128)

        self.k = k

    def forward(self, x):
        batchsize = x.size()[0]
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.relu(self.bn3(self.conv3(x)))
        x = torch.max(x, 2, keepdim=True)[0]
        x = x.view(-1, 512)

        x = F.relu(self.bn4(self.fc1(x)))
        x = F.relu(self.bn5(self.fc2(x)))
        x = self.fc3(x)

        iden = Variable(torch.from_numpy(np.eye(self.k).flatten().astype(np.float32))).view(1, self.k * self.k).repeat(
            batchsize, 1)
        if x.is_cuda:
            iden = iden.to(x.get_device())
        x = x + iden
        x = x.view(-1, self.k, self.k)
        return x


class MeshSegNet(nn.Module):
    def __init__(self, num_classes=15, num_channels=15, with_dropout=True, dropout_p=0.5):
        super(MeshSegNet, self).__init__()
        self.num_classes = num_classes
        self.num_channels = num_channels
        self.with_dropout = with_dropout
        self.dropout_p = dropout_p

        # MLP-1 [64, 64]
        self.mlp1_conv1 = torch.nn.Conv1d(self.num_channels, 64, 1)
        self.mlp1_conv2 = torch.nn.Conv1d(64, 64, 1)
        self.mlp1_bn1 = nn.BatchNorm1d(64)
        self.mlp1_bn2 = nn.BatchNorm1d(64)
        # FTM (feature-transformer module)
        self.fstn = STNkd(k=64)
        # GLM-1 (graph-contrained learning modulus)
        self.glm1_conv1_1 = torch.nn.Conv1d(64, 32, 1)
        self.glm1_conv1_2 = torch.nn.Conv1d(64, 32, 1)
        self.glm1_bn1_1 = nn.BatchNorm1d(32)
        self.glm1_bn1_2 = nn.BatchNorm1d(32)
        self.glm1_conv2 = torch.nn.Conv1d(32 + 32, 64, 1)
        self.glm1_bn2 = nn.BatchNorm1d(64)
        # MLP-2
        self.mlp2_conv1 = torch.nn.Conv1d(64, 64, 1)
        self.mlp2_bn1 = nn.BatchNorm1d(64)
        self.mlp2_conv2 = torch.nn.Conv1d(64, 128, 1)
        self.mlp2_bn2 = nn.BatchNorm1d(128)
        self.mlp2_conv3 = torch.nn.Conv1d(128, 512, 1)
        self.mlp2_bn3 = nn.BatchNorm1d(512)
        # GLM-2 (graph-contrained learning modulus)
        self.glm2_conv1_1 = torch.nn.Conv1d(512, 128, 1)
        self.glm2_conv1_2 = torch.nn.Conv1d(512, 128, 1)
        self.glm2_conv1_3 = torch.nn.Conv1d(512, 128, 1)
        self.glm2_bn1_1 = nn.BatchNorm1d(128)
        self.glm2_bn1_2 = nn.BatchNorm1d(128)
        self.glm2_bn1_3 = nn.BatchNorm1d(128)
        self.glm2_conv2 = torch.nn.Conv1d(128 * 3, 512, 1)
        self.glm2_bn2 = nn.BatchNorm1d(512)
        # MLP-3
        self.mlp3_conv1 = torch.nn.Conv1d(64 + 512 + 512 + 512, 256, 1)
        self.mlp3_conv2 = torch.nn.Conv1d(256, 256, 1)
        self.mlp3_bn1_1 = nn.BatchNorm1d(256)
        self.mlp3_bn1_2 = nn.BatchNorm1d(256)
        self.mlp3_conv3 = torch.nn.Conv1d(256, 128, 1)
        self.mlp3_conv4 = torch.nn.Conv1d(128, 128, 1)
        self.mlp3_bn2_1 = nn.BatchNorm1d(128)
        self.mlp3_bn2_2 = nn.BatchNorm1d(128)
        # output
        self.output_conv = torch.nn.Conv1d(128, self.num_classes, 1)
        if self.with_dropout:
            self.dropout = nn.Dropout(p=self.dropout_p)

    def forward(self, x, A_S, A_L):

        batchsize = x.size()[0]
        n_pts = x.size()[2]
        # MLP-1
        x = F.relu(self.mlp1_bn1(self.mlp1_conv1(x)))
        x = F.relu(self.mlp1_bn2(self.mlp1_conv2(x)))
        # FTM
        trans_feat = self.fstn(x)
        x = x.transpose(2, 1)
        x_ftm = torch.bmm(x, trans_feat)
        # GLM-1
        sap = torch.bmm(A_S, x_ftm)
        sap = sap.transpose(2, 1)
        x_ftm = x_ftm.transpose(2, 1)
        x = F.relu(self.glm1_bn1_1(self.glm1_conv1_1(x_ftm)))
        glm_1_sap = F.relu(self.glm1_bn1_2(self.glm1_conv1_2(sap)))
        x = torch.cat([x, glm_1_sap], dim=1)
        x = F.relu(self.glm1_bn2(self.glm1_conv2(x)))
        # MLP-2
        x = F.relu(self.mlp2_bn1(self.mlp2_conv1(x)))
        x = F.relu(self.mlp2_bn2(self.mlp2_conv2(x)))
        x_mlp2 = F.relu(self.mlp2_bn3(self.mlp2_conv3(x)))
        if self.with_dropout:
            x_mlp2 = self.dropout(x_mlp2)
        # GLM-2
        x_mlp2 = x_mlp2.transpose(2, 1)
        sap_1 = torch.bmm(A_S, x_mlp2)
        sap_2 = torch.bmm(A_L, x_mlp2)
        x_mlp2 = x_mlp2.transpose(2, 1)
        sap_1 = sap_1.transpose(2, 1)
        sap_2 = sap_2.transpose(2, 1)
        x = F.relu(self.glm2_bn1_1(self.glm2_conv1_1(x_mlp2)))
        glm_2_sap_1 = F.relu(self.glm2_bn1_2(self.glm2_conv1_2(sap_1)))
        glm_2_sap_2 = F.relu(self.glm2_bn1_3(self.glm2_conv1_3(sap_2)))
        x = torch.cat([x, glm_2_sap_1, glm_2_sap_2], dim=1)
        x_glm2 = F.relu(self.glm2_bn2(self.glm2_conv2(x)))
        # GMP
        x = torch.max(x_glm2, 2, keepdim=True)[0]
        # Upsample
        x = torch.nn.Upsample(n_pts)(x)
        # Dense fusion
        x = torch.cat([x, x_ftm, x_mlp2, x_glm2], dim=1)
        # MLP-3
        x = F.relu(self.mlp3_bn1_1(self.mlp3_conv1(x)))
        x = F.relu(self.mlp3_bn1_2(self.mlp3_conv2(x)))
        x = F.relu(self.mlp3_bn2_1(self.mlp3_conv3(x)))
        if self.with_dropout:
            x = self.dropout(x)
        x = F.relu(self.mlp3_bn2_2(self.mlp3_conv4(x)))
        # output
        x = self.output_conv(x)
        x = x.permute(0, 2, 1)

        return x


class LitMeshSegNet(pl.LightningModule):
    def __init__(self, pc_size, in_channels=12, output_channels=8):
        super().__init__()
        self.net = MeshSegNet(num_classes=output_channels)
        self.loss_f = nn.CrossEntropyLoss()
        self.accuracy = torchmetrics.Accuracy(task="multiclass", num_classes=output_channels, num_labels=pc_size)
        self.miou = torchmetrics.JaccardIndex(task="multiclass", num_classes=output_channels, num_labels=pc_size)
        self.confusion_matrix = torchmetrics.ConfusionMatrix(task="multiclass", num_classes=output_channels,
                                                             num_labels=pc_size)
        self.matthews_c_c = torchmetrics.MatthewsCorrCoef(task="multiclass", num_classes=output_channels,
                                                          num_labels=pc_size)
        self.dice_score = torchmetrics.Dice(num_classes=output_channels, average='macro')

    def get_data(self, x):
        with torch.no_grad():
            x = x.transpose(2, 1)
            A_S = torch.zeros(x.shape[0], x.shape[1], x.shape[1]).float().to(x.device)
            A_L = torch.zeros(x.shape[0], x.shape[1], x.shape[1]).float().to(x.device)

            D = torch.cdist(x[:, :, 9:12], x[:, :, 9:12])
            A_S[D < 0.1] = 1.0
            A_S = A_S / torch.bmm(torch.sum(A_S, dim=2, keepdim=True), torch.ones(x.shape[0], 1, x.shape[1]).to(x.device))

            A_L[D < 0.2] = 1.0
            A_L = A_L / torch.bmm(torch.sum(A_L, dim=2, keepdim=True), torch.ones(x.shape[0], 1, x.shape[1]).to(x.device))

            x = x.transpose(2, 1)
            return x, A_S, A_L

    def predict(self, data):
        x, mesh_triangles, labels = data
        x = x.transpose(2, 1)
        with torch.no_grad():
            x, A_S, A_L = self.get_data(x)
            out = self.net(x, A_S, A_L)
        out = torch.exp(torch.nn.functional.log_softmax(out.transpose(2, 1), dim=1))[0]
        labels_pred = out.max(0)[1]
        return labels_pred

    def validate(self, data):
        x, mesh_triangles, labels = data
        x = x.transpose(2, 1)
        with torch.no_grad():
            x, A_S, A_L = self.get_data(x)
            out = self.net(x, A_S, A_L)
        out = out.transpose(2, 1)
        self.accuracy(out, labels)
        self.miou(out, labels)
        self.matthews_c_c(out, labels)
        self.dice_score(out, labels)
        metrics = {
            'final_accuracy': self.accuracy.compute(),
            'final_miou': self.miou.compute(),
            'final_dice_score': self.dice_score.compute(),
            'final_matthews_c_c': self.matthews_c_c.compute()
        }
        self.accuracy.reset()
        self.miou.reset()
        self.dice_score.reset()
        self.matthews_c_c.reset()
        return metrics

    def training_step(self, batch, batch_idx):
        x, mesh_triangles, labels = batch
        x = x.transpose(2, 1)
        x, A_S, A_L = self.get_data(x)
        out = self.net(x, A_S, A_L)
        out = out.transpose(2, 1)
        loss = self.loss_f(out, labels)
        sch = self.lr_schedulers()
        # step every N epochs
        if self.trainer.is_last_batch:
            sch.step()
        self.log("train_loss", loss, prog_bar=True, logger=True)
        return loss

    def validation_step(self, batch, batch_idx):
        x, mesh_triangles, labels = batch
        x = x.transpose(2, 1)
        x, A_S, A_L = self.get_data(x)
        out = self.net(x, A_S, A_L)
        out = out.transpose(2, 1)
        loss = self.loss_f(out, labels)
        self.accuracy(out.detach(), labels)
        self.miou(out.detach(), labels)
        self.matthews_c_c(out.detach(), labels)
        self.dice_score(out.detach(), labels)
        self.log('val_seg_acc', self.accuracy, prog_bar=False, logger=True)
        self.log('val_seg_mat_c_c', self.matthews_c_c, prog_bar=False, logger=True)
        self.log('val_miou', self.miou, prog_bar=False, logger=True)
        self.log('val_dice', self.dice_score, prog_bar=False, logger=True)
        self.log("val_loss", loss, prog_bar=True, logger=True)
        return loss

    def test_step(self, batch, batch_idx):
        metrics = self.validate(batch)
        self.log_dict(metrics)
        labels_pred = self.predict(batch).cpu().numpy()
        mesh = trimesh.Trimesh(**trimesh.triangles.to_kwargs(batch[1][0].cpu().detach().numpy()))
        colors = label_to_colors(labels_pred)
        mesh.visual.face_colors = colors
        mesh_path = f'{os.path.join(self.logger.log_dir, "meshes")}'
        Path(mesh_path).mkdir(parents=True, exist_ok=True)
        mesh.export(f'{os.path.join(self.logger.log_dir, "meshes")}/mesh_{batch_idx}_{self.global_rank}.ply')

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3, betas=(0.9, 0.999), weight_decay=1e-5)
        sch = torch.optim.lr_scheduler.StepLR(
            optimizer, step_size=60, gamma=0.5, verbose=True)
        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": sch,
                "monitor": "train_loss",
            }
        }
